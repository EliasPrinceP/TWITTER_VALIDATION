# -*- coding: utf-8 -*-
"""NLP 3 TWITTER VALIDATION

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wlszQ-GoChqohImTCZ-J9EWAKHPOAOeX
"""

import pandas as pd
df_cols=['Id','Location','Target','Text']
df=pd.read_csv('/content/twitter_validation.csv',names=df_cols)
df

df.isna().sum()

df['Target'].value_counts()

import seaborn as sns
sns.countplot(x=df['Target'])

#to  delete rows that contains "irrelevant"
df.drop(df.index[(df['Target']=='Irrelevant')],axis=0,inplace=True)
df

df['Target'].value_counts()

#to reset the index
df.reset_index(drop=True,inplace=True)

df

df.drop(['Id','Location'],axis=1,inplace=True)

df

df['Target']=df['Target'].map({'Positive':1,'Negative':-1,'Neutral':0})

df

tweets=df.Text
tweets

# stri='hai!!!aaa###aaaa%%%wwww@#$%^'
# import re
# lst=re.findall('[^a-zA-Z0-9]+',stri)
# lst

tweets=tweets.str.replace('[^a-zA-Z0-9]+'," ")
tweets

import nltk
from nltk.stem import SnowballStemmer
from nltk import TweetTokenizer
stemmer=SnowballStemmer('english')
tk=TweetTokenizer()

# tweets=tweets.apply(lambda x:[stemmer.stem(i.lower()) for i in tk.tokenize(x)])
# tweets

tweets=tweets.apply(lambda x:[stemmer.stem(i.lower()) for i in tk.tokenize(x)]).apply(lambda y:" ".join(y))
tweets

from nltk.corpus import stopwords
nltk.download('stopwords')
sw=stopwords.words('english')
print(sw)

tweets=tweets.apply(lambda x:[i for i in tk.tokenize(x) if i not in sw]).apply(lambda y:" ".join(y))
tweets

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer=TfidfVectorizer()
train_vec=vectorizer.fit_transform(tweets)
print(train_vec)

print(train_vec.shape)

X=train_vec
y=df['Target'].values

from sklearn.model_selection import train_test_split
Xtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=.3,random_state=1)

from sklearn.svm import SVC
from sklearn.naive_bayes import  MultinomialNB
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from sklearn.metrics import ConfusionMatrixDisplay,classification_report
sv=SVC(kernel='linear')
mn=MultinomialNB()
rfc=RandomForestClassifier()
adb=AdaBoostClassifier()
lst=[sv,mn,rfc,adb]
for i in lst:
  i.fit(Xtrain,ytrain)
  ypred=i.predict(Xtest) 
  s='facebook is a hub of fake information.'
  ynew=i.predict(vectorizer.transform([s]))
  if ynew==-1:
    print("***********NEGATIVE***********")
  elif ynew==0:
    print("***********NEUTRAL************")
  else:
    print("***********POSITIVE***********")

  

  print(classification_report(ypred,ytest))
  print("________________________________________________________________________________________________")
  print(ConfusionMatrixDisplay.from_predictions(ypred,ytest))

# training
# petal_length
# petal_width
# sepal_length
# sepal_width